pipeline {
    agent { label 'performance-test' }

    parameters {
        string(
            name: 'COMPARE_VERSION',
            defaultValue: '0001',
            description: 'The checked in baseline to compare against. Will also check against previous version to guard against large inadvertent fluctuations in the baseline.'
        )
        string(
            name: 'PYTEST_OPTIONS',
            defaultValue: '-sv  --runslow --benchmark-disable-gc',
            description: 'Optional parameters for pytest, eg. --benchmark-cprofile=tottime --benchmark-storage=file:///scratch/oompf/jenkins-ert-performance/.benchmarks'
        )
    }

    environment {
        BASE_DIR = '/scratch/oompf/jenkins-ert-performance'
        TMP_DIR = """${sh(script:"mktemp -dp ${env.BASE_DIR}", returnStdout: true).trim()}"""
    }

    stages {
        stage('install') {
            steps {
                sh """
                python3 -m venv env
                source env/bin/activate
                source /opt/rh/devtoolset-9/enable
                pip install --upgrade pip
                pip install .
                pip install -r dev-requirements.txt
                """
            }
        }
        stage('run benchmarks') {
            steps {
                sh """
                source env/bin/activate
                pytest tests/performance_tests \
                  --benchmark-save=benchmark-jenkins \
                  --benchmark-compare=${COMPARE_VERSION} \
                  --benchmark-compare-fail=median:15% \
                  --benchmark-autosave \
                  --basetemp=${env.TMP_DIR} \
                  --benchmark-min-rounds=25 \
                  --benchmark-sort=fullname \
                  --benchmark-warmup=on \
                  ${PYTEST_OPTIONS}
                """
            }
        }
    }
    post {
        always {
            archiveArtifacts(
                artifacts: ".benchmarks/**"
            )
        }
    }
}
